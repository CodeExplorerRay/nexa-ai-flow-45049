*** Begin Patch
*** Add File: server/package.json
+{
+  "name": "nexa-ai-server",
+  "version": "0.1.0",
+  "private": true,
+  "main": "src/app.js",
+  "scripts": {
+    "start": "node src/app.js",
+    "dev": "nodemon src/app.js",
+    "test": "jest"
+  },
+  "dependencies": {
+    "express": "^4.18.2",
+    "node-fetch": "^2.6.7",
+    "helmet": "^6.0.1",
+    "express-rate-limit": "^6.7.0",
+    "dotenv": "^16.0.3"
+  },
+  "devDependencies": {
+    "nodemon": "^2.0.22",
+    "jest": "^29.5.0",
+    "supertest": "^6.3.3"
+  }
+}
*** End Patch
*** Begin Patch
*** Add File: server/src/app.js
+const express = require("express");
+const helmet = require("helmet");
+const rateLimit = require("express-rate-limit");
+const fetch = require("node-fetch");
+require("dotenv").config();
+
+const app = express();
+app.use(express.json());
+app.use(helmet());
+
+const limiter = rateLimit({
+  windowMs: 60 * 1000,
+  max: 30,
+});
+app.use(limiter);
+
+const OLLAMA_HOST = process.env.OLLAMA_HOST || "http://localhost:11434";
+
+app.post("/api/generate", async (req, res) => {
+  try {
+    const { prompt } = req.body;
+    if (!prompt) return res.status(400).json({ ok: false, error: "Missing prompt" });
+
+    const response = await fetch(`${OLLAMA_HOST}/api/generate`, {
+      method: "POST",
+      headers: { "Content-Type": "application/json" },
+      body: JSON.stringify({ model: process.env.OLLAMA_MODEL || "llama3.2", prompt }),
+    });
+
+    const data = await response.json();
+    res.json({ ok: true, data });
+  } catch (err) {
+    console.error("Generation error:", err);
+    res.status(500).json({ ok: false, error: err.message });
+  }
+});
+
+const PORT = process.env.PORT || 3001;
+app.listen(PORT, () => console.log(`Ollama proxy running on port ${PORT}`));
+
+module.exports = app;
*** End Patch
